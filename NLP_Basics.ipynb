{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk  -----------NLP Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anacondo command prompt\n",
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To check whether nltk is installed or not\n",
    "# from nltk.corpus import brown\n",
    "# brown.words()\n",
    "# brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I learned that if an electric slicer is used t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But they don't clean the chiles?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0                           Wow... Loved this place.        1.0\n",
       "1  I learned that if an electric slicer is used t...        NaN\n",
       "2                   But they don't clean the chiles?        NaN\n",
       "3                                 Crust is not good.        0.0\n",
       "4          Not tasty and the texture was just nasty.        0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"User_reviews (1).csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3729, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I learned that if an electric slicer is used t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But they don't clean the chiles?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0                           Wow... Loved this place.        1.0\n",
       "1  I learned that if an electric slicer is used t...        NaN\n",
       "2                   But they don't clean the chiles?        NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usernew=data[0:3]\n",
    "usernew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learned that if an electric slicer is used the blade becomes hot enough to start to cook the prosciutto.\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "\n",
    "\n",
    "example_text=usernew[\"Review\"][1]\n",
    "\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I learned that if an electric slicer is used the blade becomes hot enough to start to cook the prosciutto.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokentize   # full stop for news sentence identifier\n",
    "\n",
    "sent_tokens=sent_tokenize(example_text)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learned', 'that', 'if', 'an', 'electric', 'slicer', 'is', 'used', 'the', 'blade', 'becomes', 'hot', 'enough', 'to', 'start', 'to', 'cook', 'the', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "# word Tokentizer\n",
    "\n",
    "word_tokens=word_tokenize(example_text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'couldn', 'each', 'an', \"you've\", 'into', 'hasn', \"it's\", 'me', 'he', 'under', 'my', 'o', 've', 'too', 'only', 'does', 'have', 'theirs', 'having', 'between', 'y', 'of', 'i', 'own', 'so', 'can', 'further', 'our', \"hadn't\", 'll', 'd', 'not', 'you', \"won't\", 'they', 'but', 'same', 'before', 'won', 'out', 'him', 'over', 'as', 'shouldn', 'isn', 'himself', 'down', 'hers', 'ours', 'how', 'any', 'up', 'nor', 'were', 'didn', \"needn't\", \"aren't\", 'mustn', 'when', 'has', \"she's\", 'no', \"wasn't\", \"should've\", 'some', 'm', 'ourselves', 'myself', 'again', 'is', 'here', 'had', 'are', 'both', 's', 'in', 'do', \"mustn't\", 'am', 'mightn', 'she', 'there', 'don', \"haven't\", 'which', \"shouldn't\", 'while', 'if', 'other', 'hadn', 'being', 'below', \"doesn't\", 'yourselves', 'a', 'we', 'themselves', 'and', 'these', 'until', \"wouldn't\", 'all', 'should', 'whom', 'wouldn', 'weren', 're', 'was', 'after', 'by', 'who', 'herself', 'most', \"you'd\", 'them', \"hasn't\", 'shan', 'just', 'yours', 'be', \"you're\", 'ma', 'been', \"isn't\", 'wasn', \"couldn't\", \"mightn't\", 'needn', 'this', 'to', 'with', 't', 'during', 'doing', 'very', 'above', 'the', 'her', 'his', \"you'll\", 'what', 'ain', 'off', \"didn't\", 'yourself', 'because', 'or', 'then', 'about', \"weren't\", 'at', 'through', 'where', 'aren', 'against', 'will', 'that', \"shan't\", 'on', 'itself', 'your', 'such', \"that'll\", 'than', 'now', 'for', 'once', 'it', 'haven', 'why', \"don't\", 'its', 'those', 'from', 'did', 'few', 'doesn', 'more', 'their'}\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=set(stopwords.words(\"english\")) \n",
    "\n",
    "print(stop_words)\n",
    "\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learned', 'electric', 'slicer', 'used', 'blade', 'becomes', 'hot', 'enough', 'start', 'cook', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence=[]\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "{'couldn', 'each', 'an', \"you've\", 'into', 'hasn', \"it's\", 'me', 'he', 'under', 'my', 'o', 've', '}', 'too', 'only', 'does', 'have', 'theirs', 'having', 'between', '{', 'y', 'of', 'i', 'own', 'so', 'can', 'further', 'our', \"hadn't\", 'll', 'd', 'not', 'you', \"won't\", 'they', 'but', 'same', 'before', 'won', 'out', 'him', 'over', 'as', 'shouldn', 'isn', 'himself', 'down', 'hers', 'ours', 'how', 'any', 'up', 'nor', '(', 'were', 'didn', \"needn't\", \"aren't\", 'mustn', 'when', 'has', \"she's\", 'no', \"wasn't\", \"should've\", 'some', 'm', 'ourselves', 'myself', 'again', 'is', 'here', 'had', '?', 'are', 'both', 's', 'in', 'do', \"mustn't\", 'am', 'mightn', 'she', 'there', 'don', \"haven't\", 'which', \"shouldn't\", 'while', 'if', 'other', 'hadn', 'being', 'below', \"doesn't\", 'yourselves', 'a', 'we', 'themselves', 'and', 'these', 'until', \"wouldn't\", 'all', 'should', 'whom', 'wouldn', 'weren', 're', 'was', 'after', 'by', 'who', 'herself', 'most', \"you'd\", 'them', \"hasn't\", 'shan', 'just', 'yours', 'be', \"you're\", 'ma', 'been', \"isn't\", 'wasn', \"couldn't\", \"mightn't\", '...', 'needn', 'this', 'to', 'with', 't', 'during', 'doing', 'very', 'above', 'the', 'her', 'his', \"you'll\", 'what', 'ain', 'off', \"didn't\", 'yourself', 'because', 'or', 'then', 'about', \"weren't\", 'at', 'through', 'where', 'aren', 'against', 'will', 'that', \"shan't\", 'on', 'itself', 'your', 'such', '.', \"that'll\", 'than', 'now', 'for', 'once', 'it', 'haven', 'why', \"don't\", 'its', 'those', 'from', 'did', 'few', 'doesn', ')', 'more', 'their'}\n"
     ]
    }
   ],
   "source": [
    "stop_words.update([\".\",\"...\",\"?\",\"{\",\"}\",\"(\",\")\"]) #update the stop words \n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learned', 'electric', 'slicer', 'used', 'blade', 'becomes', 'hot', 'enough', 'start', 'cook', 'prosciutto']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence=[]\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learn', 'that', 'if', 'an', 'electr', 'slicer', 'is', 'use', 'the', 'blade', 'becom', 'hot', 'enough', 'to', 'start', 'to', 'cook', 'the', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "stem_token=[stemmer.stem(word) for word in word_tokens]\n",
    "\n",
    "print(stem_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learned', 'that', 'if', 'an', 'electric', 'slicer', 'is', 'used', 'the', 'blade', 'becomes', 'hot', 'enough', 'to', 'start', 'to', 'cook', 'the', 'prosciutto', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "lemm_token=[lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "\n",
    "print(lemm_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Text', 'NN'), ('mining', 'NN'), ('is', 'VBZ'), ('also', 'RB'), ('refeered', 'VBN'), ('as', 'IN'), ('text', 'NN'), ('data', 'NNS'), ('Mining', 'NNP'), (',', ','), ('rough', 'JJ'), ('equivalent', 'JJ'), ('text', 'NN'), ('analytics', 'NNS'), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('derivating', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "txt= \"Text mining is also refeered as text data Mining,rough equivalent text analytics  is the process of derivating\"\n",
    "\n",
    "wordli=nltk.word_tokenize(txt)\n",
    "\n",
    "tag=nltk.pos_tag(wordli)\n",
    "\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Apple/NNP)\n",
      "  bought/VBD\n",
      "  car/NN\n",
      "  from/IN\n",
      "  (GPE Apple/NNP)\n",
      "  store/NN\n",
      "  from/IN\n",
      "  (ORGANIZATION Stanford/NNP University/NNP))\n",
      "[('Apple', 'PERSON'), ('Apple', 'GPE'), ('Stanford University', 'ORGANIZATION')]\n"
     ]
    }
   ],
   "source": [
    "# Name Entity Recognisation\n",
    "\n",
    "doc='''\n",
    "      Apple bought car from Apple store from Stanford University '''\n",
    "\n",
    "\n",
    "\n",
    "#tokenize doc\n",
    "\n",
    "tokenizedoc=nltk.word_tokenize(doc)\n",
    "tagged_sentenc=nltk.pos_tag(tokenizedoc)\n",
    "\n",
    "nechunck=nltk.ne_chunk(tagged_sentenc)\n",
    "\n",
    "print(nechunck)\n",
    "named_entity=[]\n",
    "\n",
    "for tagged_tree in nechunck:\n",
    "    \n",
    "    if hasattr(tagged_tree,\"label\"):\n",
    "        entity_name=' '.join(c[0] for c in tagged_tree.leaves())\n",
    "        entity_type=tagged_tree.label()\n",
    "        named_entity.append((entity_name,entity_type))\n",
    "print(named_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>becomes</th>\n",
       "      <th>blade</th>\n",
       "      <th>but</th>\n",
       "      <th>chiles</th>\n",
       "      <th>clean</th>\n",
       "      <th>cook</th>\n",
       "      <th>don</th>\n",
       "      <th>electric</th>\n",
       "      <th>enough</th>\n",
       "      <th>...</th>\n",
       "      <th>prosciutto</th>\n",
       "      <th>slicer</th>\n",
       "      <th>start</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>they</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "      <th>used</th>\n",
       "      <th>wow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   an  becomes  blade  but  chiles  clean  cook  don  electric  enough  ...  \\\n",
       "0   0        0      0    0       0      0     0    0         0       0  ...   \n",
       "1   1        1      1    0       0      0     1    0         1       1  ...   \n",
       "2   0        0      0    1       1      1     0    1         0       0  ...   \n",
       "\n",
       "   prosciutto  slicer  start  that  the  they  this  to  used  wow  \n",
       "0           0       0      0     0    0     0     1   0     0    1  \n",
       "1           1       1      1     1    2     0     0   2     1    0  \n",
       "2           0       0      0     0    1     1     0   0     0    0  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorisation\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvect1=CountVectorizer()\n",
    "\n",
    "dtm1=pd.DataFrame(countvect1.fit_transform(usernew[\"Review\"]).toarray(),columns=countvect1.get_feature_names())\n",
    "\n",
    "dtm1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>becomes</th>\n",
       "      <th>blade</th>\n",
       "      <th>but</th>\n",
       "      <th>chiles</th>\n",
       "      <th>clean</th>\n",
       "      <th>cook</th>\n",
       "      <th>don</th>\n",
       "      <th>electric</th>\n",
       "      <th>enough</th>\n",
       "      <th>...</th>\n",
       "      <th>prosciutto</th>\n",
       "      <th>slicer</th>\n",
       "      <th>start</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>they</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "      <th>used</th>\n",
       "      <th>wow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.216607</td>\n",
       "      <td>0.216607</td>\n",
       "      <td>0.216607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216607</td>\n",
       "      <td>0.216607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216607</td>\n",
       "      <td>0.216607</td>\n",
       "      <td>0.216607</td>\n",
       "      <td>0.216607</td>\n",
       "      <td>0.329470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.433213</td>\n",
       "      <td>0.216607</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322002</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         an   becomes     blade       but    chiles     clean      cook  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.216607  0.216607  0.216607  0.000000  0.000000  0.000000  0.216607   \n",
       "2  0.000000  0.000000  0.000000  0.423394  0.423394  0.423394  0.000000   \n",
       "\n",
       "        don  electric    enough  ...  prosciutto    slicer     start  \\\n",
       "0  0.000000  0.000000  0.000000  ...    0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.216607  0.216607  ...    0.216607  0.216607  0.216607   \n",
       "2  0.423394  0.000000  0.000000  ...    0.000000  0.000000  0.000000   \n",
       "\n",
       "       that       the      they  this        to      used  wow  \n",
       "0  0.000000  0.000000  0.000000   0.5  0.000000  0.000000  0.5  \n",
       "1  0.216607  0.329470  0.000000   0.0  0.433213  0.216607  0.0  \n",
       "2  0.000000  0.322002  0.423394   0.0  0.000000  0.000000  0.0  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF -IDF vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "countvect2=TfidfVectorizer()\n",
    "dtm2=pd.DataFrame(countvect2.fit_transform(usernew[\"Review\"]).toarray(),columns=countvect2.get_feature_names())\n",
    "dtm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
