{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"/usr/local/lib/python3.7/site.py\", line 177\n",
      "    file=sys.stderr)\n",
      "        ^\n",
      "SyntaxError: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am learning Natural Language Processing. I am fond of it. I like to make my career in the same field.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I am learning Natural Language Processing. I am fond of it. I like to make my career in the same field.'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the nltk library\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am learning Natural Language Processing.',\n",
       " 'I am fond of it.',\n",
       " 'I like to make my career in the same field.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My email address is waj@simplilearn.com.', 'I live in Hyderabad']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.sent_tokenize('My email address is waj@simplilearn.com. I live in Hyderabad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am learning Natural Language Processing.  :  ['I', 'am', 'learning', 'Natural', 'Language', 'Processing', '.']\n",
      "I am fond of it.  :  ['I', 'am', 'fond', 'of', 'it', '.']\n",
      "I like to make my career in the same field.  :  ['I', 'like', 'to', 'make', 'my', 'career', 'in', 'the', 'same', 'field', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentences in nltk.tokenize.sent_tokenize(text):\n",
    "    print(sentences,' : ',nltk.tokenize.word_tokenize(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Michael',\n",
       " 'O',\n",
       " '`',\n",
       " 'Neil',\n",
       " 'works',\n",
       " 'at',\n",
       " 'Microsoft',\n",
       " ',',\n",
       " 'located',\n",
       " 'at',\n",
       " '45',\n",
       " 'Avenue',\n",
       " ',',\n",
       " 'United',\n",
       " 'States',\n",
       " 'of',\n",
       " 'America']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Limitations with nltk word tokenizer\n",
    "nltk.tokenize.word_tokenize('''Mr. Michael O`Neil works at Microsoft, located at 45 Avenue, United States of America''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### type of tokenizer\n",
    "\n",
    "# nltk.tokenize.TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am working for the England cricket board as analytics engineer'\n",
    "\n",
    "# Some words which are frequently used words, most common words such as I, am, for, the, as\n",
    "#Stopwords - words which do not add any meaning to the sentence\n",
    "\n",
    "#Steps to remove stopwords, lowercase the text, word tokenize, filter it from the stopword dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create your stopword dictionary\n",
    "my_stopwords = ['i', 'am', 'for', 'the', 'as']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['working', 'england', 'cricket', 'board', 'analytics', 'engineer']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lowercase, tokenize and filter from the above created dictionary\n",
    "clean_text = [word for word in nltk.tokenize.word_tokenize(text.lower()) if word not in my_stopwords]\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'is',\n",
       " 'an',\n",
       " 'experienced',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'engineer',\n",
       " 'at',\n",
       " 'microsoft',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'He is an experienced Natural Language Processing Engineer at Microsoft. '\n",
    "\n",
    "clean_text = [word for word in nltk.tokenize.word_tokenize(text.lower()) if word not in my_stopwords]\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The stopwords are not exhaustive. Hence either we can keep updating our stopword list every time we encounter a stop word or use a previously available stop word list within nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Using pre-available list within nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "print(mystopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knowledgeable', 'natural', 'language', 'processing', 'engineer', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering using the nltk's stopword list\n",
    "\n",
    "text = 'He is a knowledgeable Natural Language Processing Engineer.'\n",
    "\n",
    "clean_text = [word for word in nltk.tokenize.word_tokenize(text.lower()) if word not in mystopwords]\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experience',\n",
       " '4+',\n",
       " 'years',\n",
       " 'field',\n",
       " 'nlp',\n",
       " '.',\n",
       " 'working',\n",
       " 'colleague',\n",
       " 'since',\n",
       " '2',\n",
       " 'years',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'He is having experience of 4+ years in the field of NLP. I am working with him as his colleague since 2 years.'\n",
    "\n",
    "clean_text = [word for word in nltk.tokenize.word_tokenize(text.lower()) if word not in mystopwords]\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The punctuations and the digits are not removed by stopword removal. How to remove them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digits and Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'3'.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experience',\n",
       " '4+',\n",
       " 'years',\n",
       " 'field',\n",
       " 'nlp',\n",
       " 'working',\n",
       " 'colleague',\n",
       " 'since',\n",
       " 'years']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'He is having experience of 4+ years in the field of NLP. I am working with him as his colleague since 2 years.'\n",
    "\n",
    "clean_text = [word for word in nltk.tokenize.word_tokenize(text.lower()) if word not in mystopwords]\n",
    "# Removing punctuation from the stopword removed text\n",
    "clean_text = [word for word in clean_text if word not in punctuation]\n",
    "# Removing digits\n",
    "clean_text = [word for word in clean_text if not word.isdigit()]\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'polici'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('policies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'revolut'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('revolution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'better'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('better',pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'better'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experi', '4+', 'year', 'field', 'nlp', 'work', 'colleagu', 'sinc', 'year']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implementing Stemming\n",
    "\n",
    "text = 'He is having experience of 4+ years in the field of NLP. I am working with him as his colleague since 2 years.'\n",
    "\n",
    "clean_text = [word for word in nltk.tokenize.word_tokenize(text.lower()) if word not in mystopwords]\n",
    "# Removing punctuation from the stopword removed text\n",
    "clean_text = [word for word in clean_text if word not in punctuation]\n",
    "# Removing digits\n",
    "clean_text = [word for word in clean_text if not word.isdigit()]\n",
    "# Stemming of the text\n",
    "clean_text = [stemmer.stem(word) for word in clean_text]\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experience',\n",
       " '4+',\n",
       " 'years',\n",
       " 'field',\n",
       " 'nlp',\n",
       " 'work',\n",
       " 'colleague',\n",
       " 'since',\n",
       " 'years']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implementing Lemmatization\n",
    "\n",
    "text = 'He is having experience of 4+ years in the field of NLP. I am working with him as his colleague since 2 years.'\n",
    "\n",
    "clean_text = [word for word in nltk.tokenize.word_tokenize(text.lower()) if word not in mystopwords]\n",
    "# Removing punctuation from the stopword removed text\n",
    "clean_text = [word for word in clean_text if word not in punctuation]\n",
    "# Removing digits\n",
    "clean_text = [word for word in clean_text if not word.isdigit()]\n",
    "# Stemming of the text\n",
    "clean_text = [lemmatizer.lemmatize(word,pos='v') for word in clean_text]\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP True False proper noun\n",
      "is be AUX VBZ True True auxiliary\n",
      "looking look VERB VBG True False verb\n",
      "at at ADP IN True True adposition\n",
      "buying buy VERB VBG True False verb\n",
      "U.K. U.K. PROPN NNP False False proper noun\n",
      "startup startup NOUN NN True False noun\n",
      "for for ADP IN True True adposition\n",
      "$ $ SYM $ False False symbol\n",
      "1 1 NUM CD False False numeral\n",
      "billion billion NUM CD True False numeral\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.is_alpha, token.is_stop, spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple', 'looking', 'buying', 'U.K.', 'startup', '$', '1', 'billion']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in doc if token.is_stop==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG Companies, agencies, institutions, etc.\n",
      "U.K. 27 31 GPE Countries, cities, states\n",
      "$1 billion 44 54 MONEY Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_, spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An an DET DT True True determiner\n",
      "apple apple NOUN NN True False noun\n",
      "a a DET DT True True determiner\n",
      "day day NOUN NN True False noun\n",
      "keeps keep VERB VBZ True False verb\n",
      "a a DET DT True True determiner\n",
      "doctor doctor NOUN NN True False noun\n",
      "away away ADV RB True False adverb\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"An apple a day keeps a doctor away\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.is_alpha, token.is_stop, spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_, spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
